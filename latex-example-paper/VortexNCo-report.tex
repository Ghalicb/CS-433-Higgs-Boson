\documentclass[10pt,conference,compsocconf]{IEEEtran}

\usepackage{hyperref}
\usepackage{graphicx}	% For figure environment

% For the figures
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}
\usepackage{changepage}
\usepackage{tikz}
\usepackage{pgf}
\usetikzlibrary{graphs, shapes}




\begin{document}
\title{Writing Scientific Papers and Software}

\author{
  Cheng Soon Ong\\
  \textit{Department of Computer Science, ETH Zurich, Switzerland}
}
\maketitle

\begin{abstract}
  % A critical part of scientific discovery is the
  % communication of research findings to peers or the general public.
  % Mastery of the process of scientific communication improves the
  % visibility and impact of research. While this guide is a necessary
  % tool for learning how to write in a manner suitable for publication
  % at a scientific venue, it is by no means sufficient, on its own, to
  % make its reader an accomplished writer. 
  % This guide should be a starting point for further development of 
  % writing skills.
\end{abstract}

\section{Introduction}

\section{Models and Methods}

Cleaning the data was important as many columns were missing a significant amount of data (up to 70 \% for 7 of the 29 features).

\begin{figure}
  \centering
  % \resizebox{\columnwidth}{4cm}{%
  \input{figures/convergence.tex}
  % }
  \vspace{-3mm}
  \caption{Convergence of regularized logistic regression for $\lambda = 0.1$ and two
  different values of $\gamma$.}
  \label{fig:test}
\end{figure}


One important step was deciding how many iterations to perform in our
SGD. We evaluated this by plotting the training and validation errors
at each epoch for given algorithms and hyperparameters.
We chose an arbitrary value for the regularisation parameter $\lambda$ and carried out SGD with regularised logistic regression with two learning rates $\gamma$.
In both cases, the errors seemed to have converged after 1000 iterations, so we kept this value in our subsequent tests.
The result can
be seen in \autoref{fig:test}.

This proved to be a difficult decision since setting a lower learning rate can lead to better results,
if the number of iterations is high enough.

However, increasing the number of iterations can be
time-consuming, especially when there are other
factors to compare such as the cleaning method, the learning algorithm or 
the regularisation parameter.


With polynomials of higher degrees, interactions terms become very numerous hence we decided to remove them
to prevent overfitting.


We started by randomly partitioning the data into a training and test set,
putting 80\% of the data (or 200000 points) in the former.

The process then consisted in performing a three-dimensional grid-search, on a selection of $\lambda$, $\gamma$ and polynomial maximum degree.


\begin{figure}
  \centering
  % \resizebox{\columnwidth}{4cm}{%
  \input{figures/overfitting.tex}
  % }
  \vspace{-3mm}
  \caption{Validation error depending on degree of polynomial using closed-form least squares weights; adding more monomial terms leads to overfitting.}
  \label{fig:overfitting}
\end{figure}

Left out least squares gradient descent because the number of iterations was too large compared to the time it took to achieve a run.


\begin{table}
  % \centering
  % \textrm
  \begin{tabular}{ |c|c|c|c|c|c| } 
    \hline
     & LS & Ridge & LS SGD & Logistic & Reg. Logistic \\
    \hline
    \texttt{NaN} replaced & 0.0690 & 0.0710 & 0.0969 & 1.8184 & 0.6332 \\
    \texttt{NaN} removed  & 0.0759 & 0.0785 & 0.0926 & 1.1623 & 0.5384 \\
    \hline
  \end{tabular}

  % \vspace{-3mm}
  \caption{Best average validation error (MSE) over 4 folds, for different data models and learning algorithms.}
  \label{tbl:validation}
\end{table}


\begin{table}
  % \centering
  % \textrm
  \begin{tabular}{ |c|c|c|c|c|c| } 
    \hline
     & LS & Ridge & LS SGD & Logistic & Reg. Logistic \\
    \hline
    \texttt{NaN} replaced & 0.815 & 0.806 & 0.734 & 0.714 & 0.714 \\
    \texttt{NaN} removed  & 0.790 & 0.776 & 0.706 & 0.690 & 0.695 \\
    \hline
  \end{tabular}

  % \vspace{-3mm}
  \caption{Accuracy on test set using the best hyperparameters for each algorithm.}
  \label{tbl:test-accuracies}
\end{table}



\section{Results}

\section{Discussion}

\section{Conclusion}
Funny how degrees tend to not have an impact for SGD learning algorithms.
Maybe training for longer would yield better results.

% \section*{Acknowledgements}

\bibliographystyle{IEEEtran}
\bibliography{literature}

\end{document}
