\documentclass[10pt,conference,compsocconf]{IEEEtran}

\usepackage{hyperref}
\usepackage{graphicx}	% For figure environment

% For the figures
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}
\usepackage{changepage}
\usepackage{tikz}
\usepackage{pgf}
\usetikzlibrary{graphs, shapes}




\begin{document}
\title{{\LARGE Finding the Higgs Boson}\vspace{-3mm}}    

\author{
  APOSTOLOV Alexander (alexander.apostolov@epfl.ch)\\
  BAUM Auguste (auguste.baum@epfl.ch)\\
  CHRAIBI Ghali (ghali.chraibi@epfl.ch)\\
  \\
  \textit{CS-433 Machine Learning - October 2020, EPFL, Switzerland}
}
\maketitle

\begin{abstract}
  % A critical part of scientific discovery is the
  % communication of research findings to peers or the general public.
  % Mastery of the process of scientific communication improves the
  % visibility and impact of research. While this guide is a necessary
  % tool for learning how to write in a manner suitable for publication
  % at a scientific venue, it is by no means sufficient, on its own, to
  % make its reader an accomplished writer. 
  % This guide should be a starting point for further development of 
  % writing skills.
\end{abstract}

\section{Introduction}

The Higgs Boson in an elementary particle discovered in 2013 explaining why other particles have mass. This particle is not directly observable through experimentation, but one can still measure its ``decay signature'' (i.e. the products that result from its decay process). Although, it is hard to distinguish between the decay signature of a Higgs Boson and those of other particules.

Therefore, our goal here is to use binary classification techniques to predict whether a given eventâ€™s signature is the result of a Higgs boson or not.


\section{Models and Methods}

\subsection{Algorithms}

We first implemented many of the classic algorithms to solve a binary classification problem in order to compare how well they perform on our dataset. These are :
\begin{itemize}
    \item Least Square (LS) using Gradient Descent, Stochastic Gradient Descent (SGD) and its closed-form equation.
    \item Ridge Regression - closed-form equation of Least squares with a regularization term.
    \item Logistic Regression using Stochastic Gradient Descent.
    \item Regularized Logistic Regression using Stochastic Gradient Descent.
\end{itemize}

\subsection{Data Processing}

\subsubsection{Data label}
The given labels take values in \{-1, 1\}. However, it is more convenient to have labels in \{0, 1\} for (regularized) logistic regression. We therefore map all the -1 to 0.

\subsubsection{Dealing with invalid values}
The raw dataset contains many invalid value encoded as -999.0. We observed that all these invalid values were concentrated on specific features (more precisely 11 features out of 30). We considered two ways of dealing with these values :
\begin{itemize}
    \item Drop the features containing invalid values (as they represent up to 70\% of the feature).
    \item Replace the invalid values by the mean of the corresponding feature.
\end{itemize}

\subsubsection{Feature expansion}
To capture a more complex model, we use polynomial expansion (without interaction between features).

\subsubsection{Normalization}
Finally we normalized the data so that each feature has the same scale in order to achieve better convergence.

\subsection{First Model}
For our first model, we took the dataset without the columns containing invalid values and trained it using all the different algorithms mentioned above. \\
%%% Table 1 here %%%
\begin{table}[h!]
  % \centering
  % \textrm
  \begin{tabular}{ |c|c|c|c|c|c| } 
    \hline
     & LS & Ridge & LS SGD & Logistic & Reg. Logistic \\
    \hline
    \texttt{NaN} replaced & 0.0690 & 0.0710 & 0.0969 & 1.8184 & 0.6332 \\
    \texttt{NaN} removed  & 0.0759 & 0.0785 & 0.0926 & 1.1623 & 0.5384 \\
    \hline
  \end{tabular}

  % \vspace{-3mm}
  \caption{Best average validation error (MSE) over 4 folds, for different data models and learning algorithms.}
  \label{tbl:validation}
\end{table}

\subsection{Second Model}
For the second model, we took the dataset where we replaced the invalid values by the mean of their corresponding feature and trained it using all the different algorithms mentioned above.

\subsection{Third Model}
For the last model,

\subsection{Cross-validation}
We used 4-Fold cross validation with grid search to select the hyperparameters for each algorithm we considered. Cross-validation was run on the train set (80\% of the data) as we used the remaining 20\% for comparing the tuned algorithms. 

\begin{figure}
  \centering
  % \resizebox{\columnwidth}{4cm}{%
  \input{figures/convergence.tex}
  % }
  \vspace{-3mm}
  \caption{Convergence of regularized logistic regression for $\lambda = 0.1$ and two
  different values of $\gamma$.}
  \label{fig:test}
\end{figure}


One important step was deciding how many iterations to perform in our
SGD. We evaluated this by plotting the training and validation errors
at each epoch for given algorithms and hyperparameters.
We chose an arbitrary value for the regularisation parameter $\lambda$ and carried out SGD with regularised logistic regression with two learning rates $\gamma$.
In both cases, the errors seemed to have converged after 1000 iterations, so we used a value of 10000 in our subsequent tests.
The result can
be seen in \autoref{fig:test}.

This proved to be a difficult decision since setting a lower learning rate can lead to better results,
if the number of iterations is high enough.

However, increasing the number of iterations can be
time-consuming, especially when there are other
factors to compare such as the cleaning method, the learning algorithm or 
the regularisation parameter.


With polynomials of higher degrees, interactions terms become very numerous hence we decided to remove them
to prevent overfitting.


We started by randomly partitioning the data into a training and test set,
putting 80\% of the data (or 200000 points) in the former.

The process then consisted in performing a three-dimensional grid-search, on a selection of $\lambda$, $\gamma$ and polynomial maximum degree.


\begin{figure}
  \centering
  % \resizebox{\columnwidth}{4cm}{%
  \input{figures/overfitting.tex}
  % }
  \vspace{-3mm}
  \caption{Validation error depending on degree of polynomial using closed-form least squares weights; adding more monomial terms leads to overfitting.}
  \label{fig:overfitting}
\end{figure}

We left out least squares gradient descent because the number of iterations was too large compared to the time it took to achieve convergence and we saw that for similar hyperparameters the final accuracy is similar between GD and SGD.





\begin{table}
  % \centering
  % \textrm
  \begin{tabular}{ |c|c|c|c|c|c| } 
    \hline
     & LS & Ridge & LS SGD & Logistic & Reg. Logistic \\
    \hline
    \texttt{NaN} replaced & 0.815 & 0.806 & 0.734 & 0.714 & 0.714 \\
    \texttt{NaN} removed  & 0.790 & 0.776 & 0.706 & 0.690 & 0.695 \\
    \hline
  \end{tabular}

  % \vspace{-3mm}
  \caption{Accuracy on test set using the best hyperparameters for each algorithm.}
  \label{tbl:test-accuracies}
\end{table}


\begin{table}
  \centering
  % \textrm
  \begin{tabular}{ |c|c|c| } 
    \hline
    Ridge & Logistic & Reg. Logistic \\
    \hline
    0.792 & 0.711    & 0.709 \\
    \hline
  \end{tabular}
  % \vspace{-3mm}
  \caption{Best accuracy on test set partitioned according to the categorical variable. Note that Ridge regression achieves a much higher accuracy.}
  \label{tbl:test-accuracies-partitioned}
\end{table}




\section{Results}

After grid-searching for the best hyperparameters, it seemed to surface
that ridge regression achieves the best results among all the algorithms.

Note that we use a different model depending on the category the data
point belongs in with respect to the \textsf{PRI\_jet\_num} variable.

The parameters used for each category are shown in \autoref{tbl:parameters-ridge-partitioned}.

The best predictions on AICrowd has id 92621 and reached an accuracy of 0.794.
% The value of $\lambda$ chosen corresponds to that found with grid-search,
% the results of which are visualised in %\autoref{fig:rr-grid-search}.

\begin{table}
  \centering
  % \textrm
  \begin{tabular}{ |c|c|c|c|c| } 
    \hline 
    Category   & 0  & 1  & 2  & 3  \\
    Max degree & 14 & 15 & 12 & 15 \\
    \hline
  \end{tabular}
  % \vspace{-3mm}
  \caption{Parameters used to train the ridge regression that produced 79.2\% accuracy (cf. \autoref{tbl:test-accuracies-partitioned}). The value of $\lambda$ used was $10^{-8}$.}
  \label{tbl:parameters-ridge-partitioned}
\end{table}

\section{Discussion}
There are a number of free variables that can influence the performance
of the training. The learning algorithm, the learning rate and regularisation
(when required), the maximum degree of the polynomial expansion...

This complexity lead us to make choices based on intuition; namely,
which bounds and precision to use for the grid searches.
Indeed, it is very possible that, with a lower learning rate and
different regularisations (provided that convergence is reached fast
enough), we could have achieved better results.
Unfortunately, compromises had to be made with respect to the 
time-performance trade-off, which may have lead to us to
miss very high-performing combinations of parameters.

Similarly, feature expansion was performed to a lesser degree than
what might have been required: we decided not to include interactions
terms in order to limit the number of features and the effect of
multicollinearity.

Feature expansion that are not polynomial (exponential, logarithmic, sine...)

\section{Conclusion}
Funny how degrees tend to not have an impact for SGD learning algorithms.
Maybe training for longer would yield better results.

% \section*{Acknowledgements}

\bibliographystyle{IEEEtran}
\bibliography{literature}

\end{document}
