\documentclass[10pt,conference,compsocconf]{IEEEtran}

\usepackage{hyperref}
\usepackage{graphicx}	% For figure environment

% For the figures
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}
\usepackage{changepage}
\usepackage{tikz}
\usepackage{pgf}
\usetikzlibrary{graphs, shapes}




\begin{document}
\title{Writing Scientific Papers and Software}

\author{
  Cheng Soon Ong\\
  \textit{Department of Computer Science, ETH Zurich, Switzerland}
}
\maketitle

\begin{abstract}
  % A critical part of scientific discovery is the
  % communication of research findings to peers or the general public.
  % Mastery of the process of scientific communication improves the
  % visibility and impact of research. While this guide is a necessary
  % tool for learning how to write in a manner suitable for publication
  % at a scientific venue, it is by no means sufficient, on its own, to
  % make its reader an accomplished writer. 
  % This guide should be a starting point for further development of 
  % writing skills.
\end{abstract}

\section{Introduction}

\section{Models and Methods}

Cleaning the data was important as many columns were missing a significant amount of data (up to 70 \% for 7 of the 29 features).

\begin{figure}
  \centering
  % \resizebox{\columnwidth}{4cm}{%
  \input{figures/convergence.tex}
  % }
  \vspace{-3mm}
  \caption{Convergence of regularized logistic regression for $\lambda = 0.1$ and two
  different values of $\gamma$.}
  \label{fig:test}
\end{figure}


One important step was deciding how many iterations to perform in our
SGD. We evaluated this by plotting the training and validation errors
at each epoch for given algorithms and hyperparameters.
We chose an arbitrary value for the regularisation parameter $\lambda$ and carried out SGD with regularised logistic regression with two learning rates $\gamma$.
In both cases, the errors seemed to have converged after 1000 iterations, so we kept this value in our subsequent tests.
The result can
be seen in \autoref{fig:test}.

This proved to be a difficult decision since setting a lower learning rate can lead to better results,
if the number of iterations is high enough.

However, increasing the number of iterations can be
time-consuming, especially when there are other
factors to compare such as the cleaning method, the learning algorithm or 
the regularisation parameter.


With polynomials of higher degrees, interactions terms become very numerous hence we decided to remove them
to prevent overfitting.


We started by randomly partitioning the data into a training and test set,
putting 80\% of the data (or 200000 points) in the former.

The process then consisted in performing a three-dimensional grid-search, on a selection of $\lambda$, $\gamma$ and polynomial maximum degree.


\begin{figure}
  \centering
  % \resizebox{\columnwidth}{4cm}{%
  \input{figures/overfitting.tex}
  % }
  \vspace{-3mm}
  \caption{Validation error depending on degree of polynomial using closed-form least squares weights; adding more monomial terms leads to overfitting.}
  \label{fig:overfitting}
\end{figure}

Left out least squares gradient descent because the number of iterations was too large compared to the time it took to achieve a run.


\begin{table}
  % \centering
  % \textrm
  \begin{tabular}{ |c|c|c|c|c|c| } 
    \hline
     & LS & Ridge & LS SGD & Logistic & Reg. Logistic \\
    \hline
    \texttt{NaN} replaced & 0.0690 & 0.0710 & 0.0969 & 1.8184 & 0.6332 \\
    \texttt{NaN} removed  & 0.0759 & 0.0785 & 0.0926 & 1.1623 & 0.5384 \\
    \hline
  \end{tabular}

  % \vspace{-3mm}
  \caption{Best average validation error (MSE) over 4 folds, for different data models and learning algorithms.}
  \label{tbl:validation}
\end{table}


\begin{table}
  % \centering
  % \textrm
  \begin{tabular}{ |c|c|c|c|c|c| } 
    \hline
     & LS & Ridge & LS SGD & Logistic & Reg. Logistic \\
    \hline
    \texttt{NaN} replaced & 0.815 & 0.806 & 0.734 & 0.714 & 0.714 \\
    \texttt{NaN} removed  & 0.790 & 0.776 & 0.706 & 0.690 & 0.695 \\
    \hline
  \end{tabular}

  % \vspace{-3mm}
  \caption{Accuracy on test set using the best hyperparameters for each algorithm.}
  \label{tbl:test-accuracies}
\end{table}


\begin{table}
  \centering
  % \textrm
  \begin{tabular}{ |c|c|c| } 
    \hline
    Ridge & Logistic & Reg. Logistic \\
    \hline
    0.792 & 0.711    & 0.709 \\
    \hline
  \end{tabular}
  % \vspace{-3mm}
  \caption{Best accuracy on test set partitioned according to the categorical variable. Note that Ridge regression achieves a much higher accuracy.}
  \label{tbl:test-accuracies-partitioned}
\end{table}




\section{Results}

After grid-searching for the best hyperparameters, it seemed to surface
that ridge regression achieves the best results among all the algorithms.

Note that we use a different model depending on the category the data
point belongs in with respect to the \textsf{PRI\_jet\_num} variable.

The parameters used for each category are shown in \autoref{tbl:parameters-ridge-partitioned}.
% The value of $\lambda$ chosen corresponds to that found with grid-search,
% the results of which are visualised in %\autoref{fig:rr-grid-search}.

\begin{table}
  \centering
  % \textrm
  \begin{tabular}{ |c|c|c|c|c| } 
    \hline 
    Category   & 0  & 1  & 2  & 3  \\
    Max degree & 14 & 15 & 12 & 15 \\
    \hline
  \end{tabular}
  % \vspace{-3mm}
  \caption{Parameters used to train the ridge regression that produced 79.2\% accuracy (cf. \autoref{tbl:test-accuracies-partitioned}). The value of $\lambda$ used was $10^{-8}$.}
  \label{tbl:parameters-ridge-partitioned}
\end{table}

\section{Discussion}
There are a number of free variables that can influence the performance
of the training. The learning algorithm, the learning rate and regularisation
(when required), the maximum degree of the polynomial expansion...

This complexity lead us to make choices based on intuition; namely,
which bounds and precision to use for the grid searches.
Indeed, it is very possible that, with a lower learning rate and
different regularisations (provided that convergence is reached fast
enough), we could have achieved better results.
Unfortunately, compromises had to be made with respect to the 
time-performance trade-off, which may have lead to us to
miss very high-performing combinations of parameters.

Similarly, feature expansion was performed to a lesser degree than
what might have been required: we decided not to include interactions
terms in order to limit the number of features and the effect of
multicollinearity.

Feature expansion that are not polynomial (exponential, logarithmic, sine...)

\section{Conclusion}
Funny how degrees tend to not have an impact for SGD learning algorithms.
Maybe training for longer would yield better results.

% \section*{Acknowledgements}

\bibliographystyle{IEEEtran}
\bibliography{literature}

\end{document}
