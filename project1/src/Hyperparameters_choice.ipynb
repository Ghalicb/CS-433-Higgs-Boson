{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *\n",
    "from cross_validation import *\n",
    "from losses import *\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH, sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.permutation(len(y))\n",
    "train_percentage = 0.8\n",
    "cutoff_idx = int(train_percentage*len(y))\n",
    "y_train = y[:cutoff_idx]\n",
    "tX_train = tX[:cutoff_idx]\n",
    "y_test = y[cutoff_idx:]\n",
    "tX_test = tX[cutoff_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Relabel the output y from {-1,1} to {0,1}\n",
    "\n",
    "y_train_0 =y_train.copy()\n",
    "y_train_0[y_train_0 == -1] = 0\n",
    "\n",
    "\n",
    "### Dealing with invalid data & Normalize data\n",
    "# a. Dataset where columns containing invalid data are dropped\n",
    "tX_train_drop_invalid = tX_train.copy()\n",
    "tX_train_drop_invalid = tX_train_drop_invalid[:, ~np.any(tX_train_drop_invalid == -999., axis=0)]\n",
    "\n",
    "tX_train_di_means = np.mean(tX_train_drop_invalid, axis=0)\n",
    "tX_train_di_stds = np.std(tX_train_drop_invalid, axis=0)\n",
    "\n",
    "tX_train_drop_invalid = (tX_train_drop_invalid - tX_train_di_means)/tX_train_di_stds\n",
    "\n",
    "# b. Dataset where columns containing invalid data are replaced by the mean of the corresponding feature\n",
    "tX_train_replace_invalid = tX_train.copy()\n",
    "tX_train_replace_invalid[tX_train_replace_invalid == -999.] = np.nan\n",
    "\n",
    "tX_train_ri_means = np.nanmean(tX_train_replace_invalid, axis=0)\n",
    "tX_train_ri_stds = np.nanstd(tX_train_replace_invalid, axis=0)\n",
    "\n",
    "tX_train_replace_invalid = (tX_train_replace_invalid - tX_train_ri_means)/tX_train_ri_stds\n",
    "tX_train_replace_invalid[np.isnan(tX_train_replace_invalid)] = 0\n",
    "\n",
    "### Preprocessing for Test set (using the value calculated in training!)\n",
    "tX_test_drop_invalid = tX_test.copy()\n",
    "tX_test_drop_invalid = tX_test_drop_invalid[:, ~np.any(tX_test == -999., axis=0)]\n",
    "tX_test_drop_invalid = (tX_test_drop_invalid - tX_train_di_means)/tX_train_di_stds\n",
    "\n",
    "tX_test_replace_invalid = tX_test.copy()\n",
    "tX_test_replace_invalid[tX_test_replace_invalid == -999.] = np.nan\n",
    "tX_test_replace_invalid = (tX_test_replace_invalid - tX_train_ri_means)/tX_train_ri_stds\n",
    "tX_test_replace_invalid[np.isnan(tX_test_replace_invalid)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    mean_x = np.mean(x, axis=0)\n",
    "    x = x - mean_x\n",
    "    std_x = np.std(x, axis=0)\n",
    "    x = x / std_x\n",
    "    return x, mean_x, std_x\n",
    "\n",
    "def standardize_test(x, mean, std):\n",
    "    x = x - mean\n",
    "    x = x / std\n",
    "    return x\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 30)\n",
      "(200000, 19)\n"
     ]
    }
   ],
   "source": [
    "print(tX_train_replace_invalid.shape)\n",
    "print(tX_train_drop_invalid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y, tx, w, threshold):\n",
    "    pred = tx@w\n",
    "    pred_class = [0 if p<threshold else 1 for p in pred]\n",
    "    n = len(y)\n",
    "    correct=0\n",
    "    for i in range(n):\n",
    "        if pred_class[i] == y[i]:\n",
    "            correct+=1\n",
    "    return correct/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_gamma_ridge_cv(y, tx, lambdas, K, max_iters, batch_size, seed):\n",
    "  \"\"\"Do K-fold cross-validation with ridge regression for each value in lambdas, at every iteration.\n",
    "  \n",
    "  Inputs:\n",
    "  y : np array\n",
    "    (N, 1) or (N,)\n",
    "  tx : np array\n",
    "    (N, D)\n",
    "  lambdas : iterable\n",
    "    Regularisation parameters for cost function\n",
    "  K : int\n",
    "    Number of folds\n",
    "  max_iters : int\n",
    "    Maxium number of iterations for SGD\n",
    "  batch_size : int\n",
    "    Size of mini-batches\n",
    "  seed : int\n",
    "    Seed for pseudo-random number generation\n",
    "  \n",
    "  Outputs:\n",
    "  w_best : np array\n",
    "    (D, len(lambdas))\n",
    "    Trained weights that produced the smallest validation error\n",
    "    over all folds, for each lambda and gamma\n",
    "  training_errors : np array\n",
    "    (K, len(lambdas))\n",
    "    Training loss for each fold, for each lambda\n",
    "  validation_errors : np array\n",
    "    (K, len(lambdas))\n",
    "    Validation loss for each fold, for each lambda\n",
    "  \"\"\"\n",
    "  y, tx = prepare_dimensions(y, tx)\n",
    "\n",
    "  N = len(y)\n",
    "  len_lambdas = len(lambdas)\n",
    "\n",
    "  initial_w = np.ones((tx.shape[1], 1))\n",
    "  w_best = np.zeros((tx.shape[1], len_lambdas))\n",
    "\n",
    "  training_errors = np.zeros((K, len_lambdas))\n",
    "  validation_errors = np.zeros((K, len_lambdas))\n",
    "  min_error = np.inf * np.ones((len_lambdas))\n",
    "\n",
    "  k_indices = build_k_indices(y, K, seed)\n",
    "\n",
    "  for k in range(K):\n",
    "    # Take all but the k-th row of tx and y\n",
    "    tx_train, y_train = map(lambda a: a[np.delete(k_indices, k).flatten()], (tx, y))\n",
    "    # Take the k-th row of tx and y\n",
    "    tx_test, y_test = map(lambda a: a[k_indices[k]], (tx, y))\n",
    "\n",
    "    for i, lambda_ in enumerate(lambdas):\n",
    "      # Train\n",
    "      w, loss_tr = ridge_regression(y, tx, lambda_)\n",
    "      # Test\n",
    "      loss_te = compute_mse_loss(y_test, tx_test, w)\n",
    "      \n",
    "      training_errors[k, i] = loss_tr\n",
    "      validation_errors[k, i] = loss_te\n",
    "\n",
    "      # Keep the weights that give the lowest loss_te\n",
    "      if loss_te < min_error[i]:\n",
    "        min_error[i] = loss_te\n",
    "        w_best[:, i] = w.ravel()\n",
    "\n",
    "  return w_best, training_errors, validation_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1436853280529944"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas = np.array([0])\n",
    "w_best, training_errors, validation_errors = lambda_gamma_ridge_cv(y_train_0, tX_train_replace_invalid, lambdas, 4, 1000, 1, 1)\n",
    "np.mean(validation_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14668950310408446"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas = np.array([0])\n",
    "w_best, training_errors, validation_errors = lambda_gamma_ridge_cv(y_train_0, tX_train_drop_invalid, lambdas, 4, 1000, 1, 1)\n",
    "np.mean(validation_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14368542590114566\n"
     ]
    }
   ],
   "source": [
    "lambdas = np.logspace(-8, -1, 8)\n",
    "w_best, training_errors, validation_errors = lambda_gamma_ridge_cv(y_train_0, tX_train_replace_invalid, lambdas, 4, 1000, 1, 1)\n",
    "val_loss_per_lambda = np.mean(validation_errors, axis=0, dtype=np.float64)\n",
    "np.argmin(val_loss_per_lambda)\n",
    "print(val_loss_per_lambda[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14668964306932888"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas = np.logspace(-8, -1, 8)\n",
    "w_best, training_errors, validation_errors = lambda_gamma_ridge_cv(y_train_0, tX_train_drop_invalid, lambdas, 4, 1000, 1, 1)\n",
    "val_loss_per_lambda = np.mean(validation_errors, axis=0, dtype=np.float64)\n",
    "np.argmin(val_loss_per_lambda)\n",
    "val_loss_per_lambda[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GD least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_gamma_sgd_cv(y, tx, algorithm, lambdas, gammas, K, max_iters, batch_size, seed):\n",
    "  \"\"\"Do K-fold cross-validation for each value in lambdas and gammas, at every iteration.\n",
    "  \n",
    "  Inputs:\n",
    "  y : np array\n",
    "    (N, 1) or (N,)\n",
    "  tx : np array\n",
    "    (N, D)\n",
    "  algorithm : string\n",
    "    The algorithm to use for training\n",
    "    Can take any value in { \"LEAST_SQUARE\" , \"LOGISTIC_REGRESSION\", \"REGULARIZED_LOGISTIC_REGRESSION\"}\n",
    "  lambdas : iterable\n",
    "    Regularisation parameters for cost function\n",
    "  gamma : iterable\n",
    "    Learning rates for SGD\n",
    "  K : int\n",
    "    Number of folds\n",
    "  max_iters : int\n",
    "    Maxium number of iterations for SGD\n",
    "  batch_size : int\n",
    "    Size of mini-batches\n",
    "  seed : int\n",
    "    Seed for pseudo-random number generation\n",
    "  \n",
    "  Outputs:\n",
    "  w_best : np array\n",
    "    (D, len(lambdas), len(lambdas))\n",
    "    Trained weights that produced the smallest validation error\n",
    "    over all folds, for each lambda and gamma\n",
    "  training_errors : np array\n",
    "    (K, len(lambdas), len(lambdas))\n",
    "    Training loss for each fold, for each lambda and gamma\n",
    "  validation_errors : np array\n",
    "    (K, len(lambdas), len(lambdas))\n",
    "    Validation loss for each fold, for each lambda and gamma\n",
    "  \"\"\"\n",
    "  # loss = loss_kinds[algorithm][0]\n",
    "  y, tx = prepare_dimensions(y, tx)\n",
    "\n",
    "  N = len(y)\n",
    "  len_lambdas = len(lambdas)\n",
    "  len_gammas = len(gammas)\n",
    "\n",
    "  initial_w = np.ones((tx.shape[1], 1))\n",
    "  w_best = np.zeros((tx.shape[1], len_lambdas, len_gammas))\n",
    "\n",
    "  training_errors = np.zeros((K, len_lambdas, len_gammas))\n",
    "  validation_errors = np.zeros((K, len_lambdas, len_gammas))\n",
    "  min_error = np.inf * np.ones((len_lambdas, len_gammas))\n",
    "\n",
    "  k_indices = build_k_indices(y, K, seed)\n",
    "\n",
    "  for k in range(K):\n",
    "    # Take all but the k-th row of tx and y\n",
    "    tx_train, y_train = map(lambda a: a[np.delete(k_indices, k).flatten()], (tx, y))\n",
    "    # Take the k-th row of tx and y\n",
    "    tx_test, y_test = map(lambda a: a[k_indices[k]], (tx, y))\n",
    "\n",
    "    for i, lambda_ in enumerate(lambdas):\n",
    "      for j, gamma in enumerate(gammas):\n",
    "        # Train\n",
    "        w, loss_tr = SGD(y_train, tx_train, initial_w, max_iters, gamma, algorithm, batch_size, lambda_)\n",
    "        # Test\n",
    "        # loss_te = loss(y_test, tx_test, w, lambda_)\n",
    "        loss_te = compute_mse_loss_regularized(y_test, tx_test, w, 0)\n",
    "        \n",
    "        training_errors[k, i, j] = loss_tr\n",
    "        validation_errors[k, i, j] = loss_te\n",
    "\n",
    "        # Keep the weights that give the lowest loss_te\n",
    "        if loss_te < min_error[i, j]:\n",
    "          min_error[i, j] = loss_te\n",
    "          w_best[:, i, j] = w.ravel()\n",
    "\n",
    "  return w_best, training_errors, validation_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares SGD minibatch =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "0.18355890250365506\n"
     ]
    }
   ],
   "source": [
    "gammas = np.logspace(-8, -1, 8)\n",
    "lambdas = np.array([0])\n",
    "w_best, training_errors, validation_errors = lambda_gamma_sgd_cv(y_train_0, tX_train_drop_invalid, \"LEAST_SQUARE\",\n",
    "                                                                 lambdas, gammas, 4, 1000, 1, 1)\n",
    "val_loss_per_lambda = np.mean(validation_errors, axis=0, dtype=np.float64).ravel()\n",
    "idx_min = np.argmin(val_loss_per_lambda)\n",
    "print(gammas[idx_min])\n",
    "print(val_loss_per_lambda[idx_min])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "0.2437525676296876\n"
     ]
    }
   ],
   "source": [
    "gammas = np.logspace(-8, -1, 8)\n",
    "lambdas = np.array([0])\n",
    "w_best, training_errors, validation_errors = lambda_gamma_sgd_cv(y_train_0, tX_train_replace_invalid, \"LEAST_SQUARE\",\n",
    "                                                                 lambdas, gammas, 4, 1000, 1, 1)\n",
    "val_loss_per_lambda = np.mean(validation_errors, axis=0, dtype=np.float64).ravel()\n",
    "idx_min = np.argmin(val_loss_per_lambda)\n",
    "print(gammas[idx_min])\n",
    "print(val_loss_per_lambda[idx_min])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares GD minibatch =N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.15022318035698273\n"
     ]
    }
   ],
   "source": [
    "gammas = np.logspace(-8, -1, 8)\n",
    "lambdas = np.array([0])\n",
    "w_best, training_errors, validation_errors = lambda_gamma_sgd_cv(y_train_0, tX_train_drop_invalid, \"LEAST_SQUARE\",\n",
    "                                                                 lambdas, gammas, 4, 100, len(y_train_0), 1)\n",
    "val_loss_per_lambda = np.mean(validation_errors, axis=0, dtype=np.float64).ravel()\n",
    "idx_min = np.argmin(val_loss_per_lambda)\n",
    "print(gammas[idx_min])\n",
    "print(val_loss_per_lambda[idx_min])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "with open('../../pickled_files/LSGD100iters', 'wb') as file:\n",
    "        pk.dump([w_best, training_errors, validation_errors], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../pickled_files/LSGD100iters', 'rb') as file:\n",
    "        w_best, training_errors, validation_errors = pk.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.1743628693119798\n"
     ]
    }
   ],
   "source": [
    "gammas = np.logspace(-8, -1, 8)\n",
    "lambdas = np.array([0])\n",
    "w_best, training_errors, validation_errors = lambda_gamma_sgd_cv(y_train_0, tX_train_replace_invalid, \"LEAST_SQUARE\",\n",
    "                                                                 lambdas, gammas, 4, 100, len(y_train_0), 1)\n",
    "val_loss_per_lambda = np.mean(validation_errors, axis=0, dtype=np.float64).ravel()\n",
    "idx_min = np.argmin(val_loss_per_lambda)\n",
    "print(gammas[idx_min])\n",
    "print(val_loss_per_lambda[idx_min])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "with open('../../pickled_files/LSGD100itersReplacedByMean', 'wb') as file:\n",
    "        pk.dump([w_best, training_errors, validation_errors], file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "1.1202573806603948\n"
     ]
    }
   ],
   "source": [
    "gammas = np.logspace(-8, -1, 8)\n",
    "lambdas = np.array([0])\n",
    "w_best, training_errors, validation_errors = lambda_gamma_sgd_cv(y_train_0, tX_train_drop_invalid, \"LOGISTIC_REGRESSION\",\n",
    "                                                                 lambdas, gammas, 4, 1000, 1, 1)\n",
    "val_loss_per_lambda = np.mean(validation_errors, axis=0, dtype=np.float64).ravel()\n",
    "idx_min = np.argmin(val_loss_per_lambda)\n",
    "print(gammas[idx_min])\n",
    "print(val_loss_per_lambda[idx_min])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "2.1215252308907826\n"
     ]
    }
   ],
   "source": [
    "gammas = np.logspace(-8, -1, 8)\n",
    "lambdas = np.array([0])\n",
    "w_best, training_errors, validation_errors = lambda_gamma_sgd_cv(y_train_0, tX_train_replace_invalid, \"LOGISTIC_REGRESSION\",\n",
    "                                                                 lambdas, gammas, 4, 1000, 1, 1)\n",
    "val_loss_per_lambda = np.mean(validation_errors, axis=0, dtype=np.float64).ravel()\n",
    "idx_min = np.argmin(val_loss_per_lambda)\n",
    "print(gammas[idx_min])\n",
    "print(val_loss_per_lambda[idx_min])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.01\n",
      "0.323223678135669\n"
     ]
    }
   ],
   "source": [
    "gammas = np.logspace(-8, -1, 8)\n",
    "lambdas = np.logspace(-8, -1, 8)\n",
    "w_best, training_errors, validation_errors = lambda_gamma_sgd_cv(y_train_0, tX_train_drop_invalid, \"REGULARIZED_LOGISTIC_REGRESSION\",\n",
    "                                                                 lambdas, gammas, 4, 1000, 1, 1)\n",
    "val_loss_per_lambda_per_gamma = np.mean(validation_errors, axis=0, dtype=np.float64)\n",
    "idx_min = np.unravel_index(np.argmin(val_loss_per_lambda_per_gamma), val_loss_per_lambda_per_gamma.shape)\n",
    "print(lambdas[idx_min[0]])\n",
    "print(gammas[idx_min[1]])\n",
    "print(val_loss_per_lambda_per_gamma[idx_min])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.01\n",
      "0.47637808771066736\n"
     ]
    }
   ],
   "source": [
    "gammas = np.logspace(-8, -1, 8)\n",
    "lambdas = np.logspace(-8, -1, 8)\n",
    "w_best, training_errors, validation_errors = lambda_gamma_sgd_cv(y_train_0, tX_train_replace_invalid, \"REGULARIZED_LOGISTIC_REGRESSION\",\n",
    "                                                                 lambdas, gammas, 4, 1000, 1, 1)\n",
    "val_loss_per_lambda_per_gamma = np.mean(validation_errors, axis=0, dtype=np.float64)\n",
    "idx_min = np.unravel_index(np.argmin(val_loss_per_lambda_per_gamma), val_loss_per_lambda_per_gamma.shape)\n",
    "print(lambdas[idx_min[0]])\n",
    "print(gammas[idx_min[1]])\n",
    "print(val_loss_per_lambda_per_gamma[idx_min])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = np.logspace(-8, -1, 8)\n",
    "lambdas = np.logspace(-8, -1, 8)\n",
    "exp_X = build_poly(tX_train_replace_invalid, 4)\n",
    "w_best, training_errors, validation_errors = lambda_gamma_sgd_cv(y_train_0, exp_X, \"REGULARIZED_LOGISTIC_REGRESSION\",\n",
    "                                                                 lambdas, gammas, 4, 1000, 1, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
