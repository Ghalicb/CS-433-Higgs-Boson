{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from losses import *\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing losses.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f_{true}(x) = x_0*-3+x_0*2+x_1*0$$\n",
    "\n",
    "The true values are:\n",
    "$$y_0 = -1, y_1=-2, y_2=25$$\n",
    "The predictions should be:\n",
    "$$pred_0 = 6$$\n",
    "$$pred_1 = 0$$\n",
    "$$pred_2 = 5$$\n",
    "$$error_0 = -7,  error_1 = -2, error_2 = 20$$\n",
    "$$loss_0 = 49,  loss_1 = 4, loss_2 = 400$$\n",
    "so the total loss should be:\n",
    "$$453/2*3 = 75.5$$\n",
    "\n",
    "the MSE gradient should be:\n",
    "$$-1/3*[-7*[1,1,1]-2*[0, -1, 2]+20*[-5, 5, 10]] = [35.6666, -31.66666, -63.0]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-8\n",
    "\n",
    "tx = np.array([[1,1,1],[0,-1, 2], [-5, 5, 10]])\n",
    "y = np.array([-3*x[0]+2*x[1] for x in tx])\n",
    "w = np.array([3, 2, 1])\n",
    "loss_mse = compute_mse_loss(y, tx, w)\n",
    "expected_loss = 75.5\n",
    "assert type(loss_mse) == float, \"type of loss is incorrect\"\n",
    "assert np.abs(loss_mse - expected_loss)<eps ,\"Expected loss is not equal to mse_loss.\"\n",
    "\n",
    "expected_gradient = np.array([35.6666666666, -31.66666666666, -63.0])\n",
    "sg = compute_mse_gradient(y, tx, w)\n",
    "assert expected_gradient.shape == sg.shape, \"Shape of gradient is not correct\"\n",
    "for i in range(len(expected_gradient)):\n",
    "    assert np.abs(expected_gradient[i]-sg[i]) < eps, \"Expected gradient is not equal to sg\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test = np.array([-100, -20, -1, 0, 1, 100, 5000])\n",
    "sig = sigmoid(test)\n",
    "expected_sigmoid = expit(test)\n",
    "assert sig.shape==expected_sigmoid.shape, \"Sigmoid shape doesn't match.\"\n",
    "for i in range(len(expected_sigmoid)):\n",
    "    assert np.abs(expected_sigmoid[i]-sig[i]) < eps, \"Sigmoid has a wrong value at {}\".format(i)\n",
    "\n",
    "expected_log_loss = -107.29766178581356\n",
    "log_loss = compute_logistic_loss(y, tx, w)\n",
    "assert type(log_loss)==float, \"Log loss has wrong type\"\n",
    "assert np.abs(log_loss-expected_log_loss)<eps, \"Wrong log-loss value\"\n",
    "\n",
    "lambda_=0.2\n",
    "expected_reg_term = 1.4\n",
    "expected_reg_log_loss = expected_log_loss+expected_reg_term\n",
    "reg_log_loss = compute_regularized_logistic_loss(y, tx, w, lambda_)\n",
    "assert type(reg_log_loss)==float, \"Reg log loss is not a float\"\n",
    "assert np.abs(reg_log_loss-expected_reg_log_loss)<eps, \"Reg log loss has a wrong value\"\n",
    "\n",
    "\n",
    "expected_log_grad = np.array([ 122.03099163, -120.53593688, -233.06940113])\n",
    "log_grad =compute_logistic_gradient(y, tx, w)\n",
    "assert expected_log_grad.shape == log_grad.shape, \"Log gradient has an incorrect shape\"\n",
    "for i in range(len(expected_log_grad)):\n",
    "    assert np.abs(expected_log_grad[i]-log_grad[i])<eps, \"Log gradient has an incorrect value at {}\".format(i)\n",
    "\n",
    "expected_reg_term_grad = np.array([0.6, 0.4, 0.2])\n",
    "expected_reg_log_grad = expected_log_grad+expected_reg_term_grad\n",
    "reg_log_grad = compute_regularized_logistic_gradient(y, tx, w, lambda_)\n",
    "assert expected_reg_log_grad.shape == reg_log_grad.shape, \"Reg log grad has an incorrect shape\"\n",
    "for i in range(len(expected_reg_log_grad)):\n",
    "    assert np.abs(expected_reg_log_grad[i]-reg_log_grad[i])<eps, \"Log gradient has an incorrect value at {}\".format(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing helpers.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import prepare_dimensions\n",
    "y_to_reshape = np.array([1,1,1])\n",
    "tx_ = np.array([1,1,1])\n",
    "y_out, tx_out_ = prepare_dimensions(y_to_reshape, tx)\n",
    "assert y_out.shape == (3,1), \"y has incorrect shape\"\n",
    "assert len(y_out)==len(tx_), \"y and tx have different lengths\"\n",
    "assert len(tx.shape) == 2, \"tx is 2-dimensional\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import build_poly\n",
    "out = build_poly(tx, 3)\n",
    "expected_shape = (3, 13)\n",
    "assert out.shape == expected_shape, \"Shape is incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing implementations.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squared GD and SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import least_squares_GD, least_squares_SGD\n",
    "lr = 0.001\n",
    "initial_w = np.array([3,2,1])\n",
    "(w, l) = least_squares_GD(y, tx, initial_w, 1, lr)\n",
    "expected_gradient = np.array([35.6666666666, -31.66666666666, -63.0])\n",
    "expected_update =lr*expected_gradient\n",
    "expected_weights = (initial_w-expected_update).reshape([-1,1])\n",
    "assert w.shape == expected_weights.shape, \"new GD weights have incorrect shape\"\n",
    "for i in range(len(expected_weights)):\n",
    "    assert np.abs(expected_weights[i]-w[i])<eps, \"New GD weight has an incorrect value at {}\".format(i)\n",
    "\n",
    "np.random.seed(0)\n",
    "#ensuring [-5, 5, 10] is used with the fixed seed\n",
    "(w, l) = least_squares_SGD(y, tx, initial_w, 1, lr)\n",
    "expected_gradient = np.array([100, -100, -200])\n",
    "expected_update =lr*expected_gradient\n",
    "expected_weights = (initial_w-expected_update).reshape([-1,1])\n",
    "assert w.shape == expected_weights.shape, \"new weights have incorrect shape\"\n",
    "for i in range(len(expected_weights)):\n",
    "    assert np.abs(expected_weights[i]-w[i])<eps, \"New weight has an incorrect value at {}\".format(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression and regularized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import logistic_regression, reg_logistic_regression\n",
    "np.random.seed(0)\n",
    "lr = 0.001\n",
    "initial_w = np.array([3,2,1])\n",
    "(w, l) = logistic_regression(y, tx, initial_w, 1, lr)\n",
    "expected_gradient = np.array([120.03346425, -120.03346425, -240.06692851])\n",
    "expected_update =lr*expected_gradient\n",
    "expected_weights = (initial_w-expected_update).reshape([-1,1])\n",
    "assert w.shape == expected_weights.shape, \"new log-loss weights have incorrect shape\"\n",
    "for i in range(len(expected_weights)):\n",
    "    assert np.abs(expected_weights[i]-w[i])<eps, \"New log-loss weight has an incorrect value at {}\".format(i)\n",
    "\n",
    "lambda_ = 0.1\n",
    "(w, l) = reg_logistic_regression(y, tx, lambda_, initial_w, 1, lr)\n",
    "expected_gradient = np.array([120.33346425, -119.83346425, -239.96692851])\n",
    "expected_update =lr*expected_gradient\n",
    "expected_weights = (initial_w-expected_update).reshape([-1,1])\n",
    "assert w.shape == expected_weights.shape, \"new reg log-loss weights have incorrect shape\"\n",
    "for i in range(len(expected_weights)):\n",
    "    assert np.abs(expected_weights[i]-w[i])<eps, \"New reg log-loss weight has an incorrect value at {}\".format(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada] *",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
