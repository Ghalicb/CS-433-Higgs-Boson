{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from implementations import *\n",
    "from helpers import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y, tx, ids = load_csv_data(\"../data/train.csv\", True)\n",
    "N, D = tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(w, loss) = least_squares(y, tx)\n",
    "lambdas = np.logspace(1e-10, 1e10)\n",
    "weights = []\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for lambda_ in lambdas:\n",
    "  (w, loss) = ridge_regression(y_train, tx, lambda_)\n",
    "  weights.append(w)\n",
    "  losses.append(loss)\n",
    "lt.semilogx(lambdas, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Boxplots for each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# +"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in range(D):\n",
    "  a_all = tx[:, i]\n",
    "  a = a_all[a_all != -999]\n",
    "  y_a = y[a_all != -999]\n",
    "  fig, ax = plt.subplots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ax.set_title('Feature {}'.format(i))\n",
    "  ax.boxplot((a[y_a == -1], a[y_a == 1]), labels=('-1', '1'), vert=False)\n",
    "# -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# # Some scatter plots of a few features\n",
    "# There are 30 features so `29*30/2 = 435` pairs. We start with those that\n",
    "# have `NaN`s so that we can decide what to do with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All data points\n",
    "df = pd.read_csv(\"../data/train.csv\")\n",
    "df.replace(-999, np.nan, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pearson = df.corr().to_numpy()\n",
    "pearson[pearson > 0.9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fst = lambda lst: map(lambda tup: tup[0], list(lst))\n",
    "snd = lambda lst: map(lambda tup: tup[1], list(lst))\n",
    "thd = lambda lst: map(lambda tup: tup[2], list(lst))\n",
    "#fst, snd, thd = map(lambda i: lambda lst: map(lambda tup: tup[i], list(lst)), [0, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which features have NaNs, and how many?\n",
    "nan_list = [ (col, i, df[col].isna().sum()) for i, col in enumerate(df.columns[2:]) ]\n",
    "#nan_list\n",
    "nan_features = filter(lambda tup: tup[-1] != 0, nan_list)\n",
    "notnan_features = filter(lambda tup: tup[-1] == 0, nan_list)\n",
    "#list(nan_features)\n",
    "#list(notnan_features)\n",
    "nan_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for nan_feature in fst(nan_features):\n",
    "  for notnan_feature in fst(notnan_features):\n",
    "    fig, ax = plt.subplots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #print('{} against {}'.format(notnan_feature, nan_feature))\n",
    "    ax.set_title('{} against {}'.format(notnan_feature, nan_feature))\n",
    "    ax.scatter(df[nan_feature], df[notnan_feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list(map(lambda tup: tup[0], list(nan_features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list(nan_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# - Examine data with regularised least squares SGD\n",
    "# (which hyperparameters give the best results?)\n",
    "# - Try to change learning rate and maximum degree of polynomial\n",
    "# - Get rid of all features that have nans\n",
    "# - Try to write a function that tests all different hyperparameters\n",
    "# at each run of the cross-validation (rather than doing\n",
    "# a CV for each param) -> is that feasible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# +\n",
    "# 1. Get rid of NaNs\n",
    "# -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no_nan_mask = ~np.any([ tx == -999 ], axis = 1).reshape(D)\n",
    "tx_no_nans = tx[ :, no_nan_mask ]\n",
    "tx_no_nans.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# +\n",
    "# 2. Write function to get best regularisation parameter\n",
    "# -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "from cross_validation import build_k_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from project1 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lambda_eval_cv(y, tx, lambdas, K, max_iters, gamma, batch_size, seed):\n",
    "  \"\"\"Do K-fold cross-validation for each value in lambdas, at every iteration.\n",
    "  \n",
    "  Inputs:\n",
    "  y : np array\n",
    "    N x 1\n",
    "  tx : np array\n",
    "    N x D\n",
    "  initial_w : np array\n",
    "    D x 1\n",
    "  lambdas : iterable\n",
    "  K : int\n",
    "    Number of folds\n",
    "  max_iters : int\n",
    "    Maxium number of iterations for SGD\n",
    "  gamma : float\n",
    "    Learning rate for SGD\n",
    "  batch_size : int\n",
    "    Size of mini-batches\n",
    "  seed : int\n",
    "    Seed for pseudo-random number generation\n",
    "  \n",
    "  Outputs:\n",
    "  tr_losses : np array\n",
    "    Training loss averaged over every iteration, for each lambda\n",
    "  te_losses : np array\n",
    "    Validation loss averaged over every iteration, for each lambda\n",
    "  \"\"\"\n",
    "  y, tx = prepare_dimensions(y, tx)\n",
    "  N = len(y)\n",
    "\n",
    "  # Get initial w using least squares\n",
    "  # initial_w, _ = least_squares(y, tx)  \n",
    "  initial_w = np.ones((tx.shape[1], 1))\n",
    "  weights = [ initial_w for i in range(len(lambdas)) ]\n",
    "  w_best = [ initial_w for i in range(len(lambdas)) ]\n",
    "\n",
    "  k_indices = build_k_indices(y, K, seed)\n",
    "\n",
    "  training_errors = [ [] for i in range(len(lambdas))]\n",
    "  validation_errors = [ [] for i in range(len(lambdas))]\n",
    "  min_error = [ np.inf for i in range(len(lambdas))]\n",
    "\n",
    "  for k in range(K):\n",
    "    # Take all but the k-th row of tx and y\n",
    "    tx_train, y_train = map(lambda a: a[np.delete(k_indices, k).flatten()], (tx, y))\n",
    "    # Take the k-th row of tx and y\n",
    "    tx_test, y_test = map(lambda a: a[k_indices[k]], (tx, y))\n",
    "\n",
    "    for i, lambda_ in enumerate(lambdas):\n",
    "      # Train\n",
    "      w, loss_tr = SGD(y_train, tx_train, initial_w, max_iters, gamma, \"REGULARIZED_LOGISTIC_REGRESSION\", batch_size, lambda_)\n",
    "      # Test\n",
    "      loss_te = compute_regularized_logistic_loss(y_test, tx_test, w, lambda_)\n",
    "      \n",
    "      weights[i] = w\n",
    "      training_errors[i].append(loss_tr)\n",
    "      validation_errors[i].append(loss_te)\n",
    "\n",
    "      # Keep the weights that give the lowest loss_te\n",
    "      if loss_te < min_error[i]:\n",
    "        min_error[i] = loss_te\n",
    "        w_best[i] = w\n",
    "\n",
    "  # training_errors = map(lambda a: np.average(np.array(a)), training_errors)\n",
    "  # validation_errors = map(lambda a: np.average(np.array(a)), validation_errors)\n",
    "  return w_best, training_errors, validation_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-6, -1, 5)\n",
    "\n",
    "w_best, training_errors, validation_errors = lambda_eval_cv(\n",
    "  y,\n",
    "  tx_no_nans,\n",
    "  lambdas,\n",
    "  K = 4,\n",
    "  max_iters = 1000,\n",
    "  gamma = 0.05,\n",
    "  batch_size = 1,\n",
    "  seed = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "w, loss_tr = SGD(y, tx, initial_w, max_iters, gamma, \"REGULARIZED_LOGISTIC_REGRESSION\", batch_size, lambda_)\n",
    "# Test\n",
    "loss_te = compute_regularized_logistic_loss(y_test, tx_test, w, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., ..., 1., 0., 1.])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want a function from which tests can be carried out;\n",
    "# I want it to do cross-validation, say, for different values of\n",
    "# lambda and degree and gamma, and a given algorithm\n",
    "# Syntax: eval(degree = 3, lambdas = np.logspace(-10, 1, 10), algo = \"REGULARIZED_LOGISTIC_REGRESSION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from project1 import tX_train_drop_invalid, tX_train_replace_invalid\n",
    "\n",
    "data_sets = {\n",
    "  \"DROP_NANS\"  : tX_train_drop_invalid,\n",
    "  \"NAN_AVGS\"   : tX_train_replace_invalid,\n",
    "  \"CATEGORIES\" : None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "# def cv_eval(**kwargs):\n",
    "def cv_eval(\n",
    "  data_set   = \"DROP_NANS\",\n",
    "  algo       = \"REGULARIZED_LOGISTIC_REGRESSION\",\n",
    "  degree     = 1,\n",
    "  degrees    = None,\n",
    "  lambda_    = 1e-10,\n",
    "  lambdas    = None,\n",
    "  gamma      = 0.1,\n",
    "  gammas     = None,\n",
    "  K          = 4,\n",
    "  max_iters  = 100,\n",
    "  batch_size = 1,\n",
    "  seed       = 1\n",
    "):\n",
    "  \"\"\" Trains and tests models for certain parameter values;\n",
    "  \"\"\"\n",
    "  if lambdas: \n",
    "    w_best, training_errors, validation_errors = lambda_eval_cv(y, tx, lambdas, K, max_iters, gamma, batch_size, seed)\n",
    "\n",
    "\n",
    "\n",
    "  return w_best, training_errors, validation_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "def lambda_gamma_eval_cv(y, tx, lambdas, gammas, K, max_iters, batch_size, seed):\n",
    "  \"\"\"Do K-fold cross-validation for each value in lambdas, at every iteration.\n",
    "  \n",
    "  Inputs:\n",
    "  y : np array\n",
    "    N x 1\n",
    "  tx : np array\n",
    "    N x D\n",
    "  lambdas : iterable\n",
    "    Regularisation parameters for cost function\n",
    "  gamma : iterable\n",
    "    Learning rates for SGD\n",
    "  K : int\n",
    "    Number of folds\n",
    "  max_iters : int\n",
    "    Maxium number of iterations for SGD\n",
    "  batch_size : int\n",
    "    Size of mini-batches\n",
    "  seed : int\n",
    "    Seed for pseudo-random number generation\n",
    "  \n",
    "  Outputs:\n",
    "  weights : np array\n",
    "    Trained weights for \n",
    "  tr_losses : np array\n",
    "    Training loss for each fold, for each lambda and gamma\n",
    "  te_losses : np array\n",
    "    Validation loss for each fold, for each lambda and gamma\n",
    "  \"\"\"\n",
    "  y, tx = prepare_dimensions(y, tx)\n",
    "  N = len(y)\n",
    "  len_lambdas = len(lambdas)\n",
    "  len_gammas = len(gammas)\n",
    "\n",
    "  initial_w = np.ones((tx.shape[1], 1))\n",
    "  w_best = np.zeros((tx.shape[1], len_lambdas, len_gammas))\n",
    "\n",
    "  training_errors = np.zeros((K, len_lambdas, len_gammas))\n",
    "  validation_errors = np.zeros((K, len_lambdas, len_gammas))\n",
    "  min_error = np.inf * np.ones((len_lambdas, len_gammas))\n",
    "\n",
    "  k_indices = build_k_indices(y, K, seed)\n",
    "\n",
    "  for k in range(K):\n",
    "    # Take all but the k-th row of tx and y\n",
    "    tx_train, y_train = map(lambda a: a[np.delete(k_indices, k).flatten()], (tx, y))\n",
    "    # Take the k-th row of tx and y\n",
    "    tx_test, y_test = map(lambda a: a[k_indices[k]], (tx, y))\n",
    "\n",
    "    for i, lambda_ in enumerate(lambdas):\n",
    "      for j, gamma in enumerate(gammas):\n",
    "        # Train\n",
    "        w, loss_tr = SGD(y_train, tx_train, initial_w, max_iters, gamma, \"REGULARIZED_LOGISTIC_REGRESSION\", batch_size, lambda_)\n",
    "        # Test\n",
    "        loss_te = compute_regularized_logistic_loss(y_test, tx_test, w, lambda_)\n",
    "        \n",
    "        # weights[i] = w\n",
    "        training_errors[k, i, j] = loss_tr\n",
    "        validation_errors[k, i, j] = loss_te\n",
    "\n",
    "        # Keep the weights that give the lowest loss_te\n",
    "        if loss_te < min_error[i, j]:\n",
    "          min_error[i, j] = loss_te\n",
    "          w_best[:, i, j] = w.ravel()\n",
    "\n",
    "  return w_best, training_errors, validation_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-6, -1, 5)\n",
    "gammas = np.logspace(-6, -1, 5)\n",
    "\n",
    "w_best, training_errors, validation_errors = lambda_gamma_eval_cv(\n",
    "  y,\n",
    "  tx_no_nans,\n",
    "  lambdas,\n",
    "  gammas,\n",
    "  K = 4,\n",
    "  max_iters = 1000,\n",
    "  batch_size = 1,\n",
    "  seed = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[624771.33037806, 152164.69076527,  19185.57181618,\n",
       "        277776.97650153, 364959.49006667],\n",
       "       [624771.33068091, 149144.65144178,  32124.23636751,\n",
       "        236981.89825162, 306403.34900747],\n",
       "       [624771.3360619 , 156035.01500659,  25314.26831659,\n",
       "        183019.36184067, 310246.89527931],\n",
       "       [624771.43174624, 143114.17188239,  46662.77706104,\n",
       "        230771.11501926, 280082.48721357],\n",
       "       [624773.12172302, 120286.0114853 , 104933.81866281,\n",
       "        211812.95327603, 289105.63468022]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lambda_gamma_sgd_cv(algorithm, lambdas, gammas, K, max_iters, batch_size, seed):\n",
    "  \"\"\"Do K-fold cross-validation for each value in lambdas and gammas, at every iteration.\n",
    "  \n",
    "  Inputs:\n",
    "  algorithm : string\n",
    "    The algorithm to use for training\n",
    "    Can take any value in { \"LEAST_SQUARE\" , \"LOGISTIC_REGRESSION\", \"REGULARIZED_LOGISTIC_REGRESSION\"}\n",
    "  lambdas : iterable\n",
    "    Regularisation parameters for cost function\n",
    "  gamma : iterable\n",
    "    Learning rates for SGD\n",
    "  K : int\n",
    "    Number of folds\n",
    "  max_iters : int\n",
    "    Maxium number of iterations for SGD\n",
    "  batch_size : int\n",
    "    Size of mini-batches\n",
    "  seed : int\n",
    "    Seed for pseudo-random number generation\n",
    "  \n",
    "  Outputs:\n",
    "  w_best : np array\n",
    "    (D, len(lambdas), len(lambdas))\n",
    "    Trained weights that produced the smallest validation error\n",
    "    over all folds, for each lambda and gamma\n",
    "  training_errors : np array\n",
    "    (K, len(lambdas), len(lambdas))\n",
    "    Training loss for each fold, for each lambda and gamma\n",
    "  validation_errors : np array\n",
    "    (K, len(lambdas), len(lambdas))\n",
    "    Validation loss for each fold, for each lambda and gamma\n",
    "  \"\"\"\n",
    "  loss = loss_kinds[algorithm]\n",
    "  y, tx = prepare_dimensions(y, tx)\n",
    "\n",
    "  N = len(y)\n",
    "  len_lambdas = len(lambdas)\n",
    "  len_gammas = len(gammas)\n",
    "\n",
    "  initial_w = np.ones((tx.shape[1], 1))\n",
    "  w_best = np.zeros((tx.shape[1], len_lambdas, len_gammas))\n",
    "\n",
    "  training_errors = np.zeros((K, len_lambdas, len_gammas))\n",
    "  validation_errors = np.zeros((K, len_lambdas, len_gammas))\n",
    "  min_error = np.inf * np.ones((len_lambdas, len_gammas))\n",
    "\n",
    "  k_indices = build_k_indices(y, K, seed)\n",
    "\n",
    "  for k in range(K):\n",
    "    # Take all but the k-th row of tx and y\n",
    "    tx_train, y_train = map(lambda a: a[np.delete(k_indices, k).flatten()], (tx, y))\n",
    "    # Take the k-th row of tx and y\n",
    "    tx_test, y_test = map(lambda a: a[k_indices[k]], (tx, y))\n",
    "\n",
    "    for i, lambda_ in enumerate(lambdas):\n",
    "      for j, gamma in enumerate(gammas):\n",
    "        # Train\n",
    "        w, loss_tr = SGD(y_train, tx_train, initial_w, max_iters, gamma, algorithm, batch_size, lambda_)\n",
    "        # Test\n",
    "        loss_te = compute_mse_loss(y_test, tx_test, w)\n",
    "        \n",
    "        training_errors[k, i, j] = loss_tr\n",
    "        validation_errors[k, i, j] = loss_te\n",
    "\n",
    "        # Keep the weights that give the lowest loss_te\n",
    "        if loss_te < min_error[i, j]:\n",
    "          min_error[i, j] = loss_te\n",
    "          w_best[:, i, j] = w.ravel()\n",
    "\n",
    "  return w_best, training_errors, validation_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def lambda_gamma_ridge_cv(lambdas, gammas, K, max_iters, batch_size, seed):\n",
    "  \"\"\"Do K-fold cross-validation with ridge regression for each value in lambdas and gammas, at every iteration.\n",
    "  \n",
    "  Inputs:\n",
    "  lambdas : iterable\n",
    "    Regularisation parameters for cost function\n",
    "  gamma : iterable\n",
    "    Learning rates for SGD\n",
    "  K : int\n",
    "    Number of folds\n",
    "  max_iters : int\n",
    "    Maxium number of iterations for SGD\n",
    "  batch_size : int\n",
    "    Size of mini-batches\n",
    "  seed : int\n",
    "    Seed for pseudo-random number generation\n",
    "  \n",
    "  Outputs:\n",
    "  w_best : np array\n",
    "    (D, len(lambdas), len(lambdas))\n",
    "    Trained weights that produced the smallest validation error\n",
    "    over all folds, for each lambda and gamma\n",
    "  training_errors : np array\n",
    "    (K, len(lambdas), len(lambdas))\n",
    "    Training loss for each fold, for each lambda and gamma\n",
    "  validation_errors : np array\n",
    "    (K, len(lambdas), len(lambdas))\n",
    "    Validation loss for each fold, for each lambda and gamma\n",
    "  \"\"\"\n",
    "  y, tx = prepare_dimensions(y, tx)\n",
    "\n",
    "  N = len(y)\n",
    "  len_lambdas = len(lambdas)\n",
    "  len_gammas = len(gammas)\n",
    "\n",
    "  initial_w = np.ones((tx.shape[1], 1))\n",
    "  w_best = np.zeros((tx.shape[1], len_lambdas, len_gammas))\n",
    "\n",
    "  training_errors = np.zeros((K, len_lambdas, len_gammas))\n",
    "  validation_errors = np.zeros((K, len_lambdas, len_gammas))\n",
    "  min_error = np.inf * np.ones((len_lambdas, len_gammas))\n",
    "\n",
    "  k_indices = build_k_indices(y, K, seed)\n",
    "\n",
    "  for k in range(K):\n",
    "    # Take all but the k-th row of tx and y\n",
    "    tx_train, y_train = map(lambda a: a[np.delete(k_indices, k).flatten()], (tx, y))\n",
    "    # Take the k-th row of tx and y\n",
    "    tx_test, y_test = map(lambda a: a[k_indices[k]], (tx, y))\n",
    "\n",
    "    for i, lambda_ in enumerate(lambdas):\n",
    "      for j, gamma in enumerate(gammas):\n",
    "        # Train\n",
    "        w, loss_tr = ridge_regression(y, tx, lambda_)\n",
    "        # Test\n",
    "        loss_te = compute_mse_loss(y_test, tx_test, w)\n",
    "        \n",
    "        training_errors[k, i, j] = loss_tr\n",
    "        validation_errors[k, i, j] = loss_te\n",
    "\n",
    "        # Keep the weights that give the lowest loss_te\n",
    "        if loss_te < min_error[i, j]:\n",
    "          min_error[i, j] = loss_te\n",
    "          w_best[:, i, j] = w.ravel()\n",
    "\n",
    "  return w_best, training_errors, validation_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "tX_train_replace_invalid = tX_train.copy()\n",
    "tX_train_replace_invalid[tX_train_replace_invalid == -999.] = np.nan\n",
    "\n",
    "# Replace NaNs by the column average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_gamma_degree_sgd_cv(algorithm, lambdas, gammas, degrees, K, max_iters, batch_size):\n",
    "  \"\"\"Do K-fold cross-validation for each value in lambdas and gammas and each degree of polynomial expansion, at every iteration.\n",
    "  \n",
    "  Inputs:\n",
    "  algorithm : string\n",
    "    The algorithm to use for training\n",
    "    Can take any value in { \"LEAST_SQUARE\" , \"LOGISTIC_REGRESSION\", \"REGULARIZED_LOGISTIC_REGRESSION\"}\n",
    "  lambdas : iterable\n",
    "    Regularisation parameters for cost function\n",
    "  gammas : iterable\n",
    "    Learning rates for SGD\n",
    "  degrees : iterable\n",
    "    Highest polynomial degree\n",
    "  K : int\n",
    "    Number of folds\n",
    "  max_iters : int\n",
    "    Maxium number of iterations for SGD\n",
    "  batch_size : int\n",
    "    Size of mini-batches\n",
    "  seed : int\n",
    "    Seed for pseudo-random number generation\n",
    "  \n",
    "  Outputs:\n",
    "  w_best : np array\n",
    "    (D, len(lambdas), len(lambdas))\n",
    "    Trained weights that produced the smallest validation error\n",
    "    over all folds, for each lambda and gamma\n",
    "  training_errors : np array\n",
    "    (K, len(lambdas), len(lambdas))\n",
    "    Training loss for each fold, for each degree, for each lambda and gamma\n",
    "  validation_errors : np array\n",
    "    (K, len(lambdas), len(lambdas))\n",
    "    Validation loss for each fold, for each degree, for each lambda and gamma\n",
    "  \"\"\"\n",
    "  loss = loss_kinds[algorithm]\n",
    "  y, tx = prepare_dimensions(y, tx)\n",
    "\n",
    "  N = len(y)\n",
    "  len_lambdas = len(degrees)\n",
    "  len_lambdas = len(lambdas)\n",
    "  len_gammas = len(gammas)\n",
    "\n",
    "  initial_w = np.ones((tx.shape[1], 1))\n",
    "  w_best = np.zeros((tx.shape[1], len_degrees, len_lambdas, len_gammas))\n",
    "\n",
    "  training_errors = np.zeros((K, len_degrees, len_lambdas, len_gammas))\n",
    "  validation_errors = np.zeros((K, len_degrees, len_lambdas, len_gammas))\n",
    "  min_error = np.inf * np.ones((len_degrees, len_lambdas, len_gammas))\n",
    "\n",
    "  k_indices = build_k_indices(y, K, seed)\n",
    "\n",
    "  for d, degree in enumerate(degrees):\n",
    "\n",
    "    # replace_nan_by_mean\n",
    "    # Make into polynomial\n",
    "    # Normalise\n",
    "    tx_poly = build_poly(tx, degree)\n",
    "\n",
    "\n",
    "    for k in range(K):\n",
    "\n",
    "\n",
    "      # Take all but the k-th row of tx and y\n",
    "      tx_train, y_train = map(lambda a: a[np.delete(k_indices, k).flatten()], (tx_poly, y))\n",
    "      # Take the k-th row of tx and y\n",
    "      tx_test, y_test = map(lambda a: a[k_indices[k]], (tx_poly, y))\n",
    "\n",
    "      for i, lambda_ in enumerate(lambdas):\n",
    "        for j, gamma in enumerate(gammas):\n",
    "          # Train\n",
    "          w, loss_tr = SGD(y_train, tx_train, initial_w, max_iters, gamma, algorithm, batch_size, lambda_)\n",
    "          # Test\n",
    "          loss_te = compute_mse_loss(y_test, tx_test, w)\n",
    "          \n",
    "          training_errors[k, d, i, j] = loss_tr\n",
    "          validation_errors[k, d, i, j] = loss_te\n",
    "\n",
    "          # Keep the weights that give the lowest loss_te\n",
    "          if loss_te < min_error[d, i, j]:\n",
    "            min_error[d, i, j] = loss_te\n",
    "            w_best[:, d, i, j] = w.ravel()\n",
    "\n",
    "  return w_best, training_errors, validation_errors"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python [conda env:ada] *",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
