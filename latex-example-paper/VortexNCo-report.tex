\documentclass[10pt,conference,compsocconf]{IEEEtran}

\usepackage{hyperref}
\usepackage{graphicx}	% For figure environment
\usepackage{makecell}	% For multiline cells in tables
% \usepackage{subfig}
\usepackage{subcaption}

% For the figures
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}
\usepackage{changepage}
\usepackage{tikz}
\usepackage{pgf}
\usetikzlibrary{graphs, shapes}




\begin{document}
\title{{\LARGE Finding the Higgs Boson}\vspace{-3mm}}    

\author{
  APOSTOLOV Alexander (alexander.apostolov@epfl.ch)\\
  BAUM Auguste (auguste.baum@epfl.ch)\\
  CHRAIBI Ghali (ghali.chraibi@epfl.ch)\\
  \\
  \textit{CS-433 Machine Learning - October 2020, EPFL, Switzerland}
}
\maketitle

\begin{abstract}
  In this paper, we use linear classification and regression models
  to analyse Higgs boson data published by CERN.
  After exploratory analysis and testing different models, we succeeded at building a ridge regression model achieving 80\% accuracy. This was done by separating the data into different parts according to categorical variables.
  
  % A critical part of scientific discovery is the
  % communication of research findings to peers or the general public.
  % Mastery of the process of scientific communication improves the
  % visibility and impact of research. While this guide is a necessary
  % tool for learning how to write in a manner suitable for publication
  % at a scientific venue, it is by no means sufficient, on its own, to
  % make its reader an accomplished writer. 
  % This guide should be a starting point for further development of 
  % writing skills.
\end{abstract}

\section{Introduction}

The Higgs boson in an elementary particle discovered in 2013 explaining why other particles have mass. This particle is not directly observable through experimentation, but one can still measure its ``decay signature'' (i.e. the products that result from its decay process). However, it is hard to distinguish between the decay signature of a Higgs boson and those of other particles.

Therefore, our goal is to use binary classification techniques to predict whether a given eventâ€™s signature originates from a Higgs boson or not.


\section{Models and Methods}

\subsection{Algorithms}\label{sec:algo}

We first implemented many of the classic algorithms to solve a binary classification problem in order to compare how well they perform on our dataset. These are:
\begin{itemize}
    \item Least Square (LS), using Gradient Descent (GD), Stochastic Gradient Descent (SGD) and its closed-form equation.
    \item Ridge Regression (closed-form equation of LS with a regularization term).
    \item Logistic Regression using SGD.
    \item Regularized Logistic Regression using SGD.
\end{itemize}

\subsection{Data Processing}

\subsubsection{Data label}
The given labels take values in \{-1, 1\}, but it is more convenient to have labels in \{0, 1\} for (regularized) logistic regression. We therefore map all the -1 to 0.

\subsubsection{Dealing with invalid values}
The raw dataset contains many invalid values encoded as -999.0. We observed that all those were concentrated in specific features (11 features out of the 30). We considered two ways of dealing with these values :
\begin{itemize}
    \item Drop the features containing invalid values (as they can represent up to 70\% of the data for those features).
    \item Replace the invalid values by the mean of the corresponding feature.
\end{itemize}

\subsubsection{Feature expansion}
To produce a more flexible model, we use polynomial expansion (without interaction between features).

\subsubsection{Normalization}
Finally we normalized the data so that each feature has the same scale in order to achieve better convergence.

\subsection{Cross-validation}
We used 4-Fold cross validation with grid search to select the hyperparameters (linear space for degree of polynomial expansion ad logarithmic space for the learning rate $\gamma$ and the regularisation term $\lambda$) for each algorithm we considered. Cross-validation was run only on 80\% of the available training data in order to use the remaining 20\% exclusively for comparing the algorithms once optimally tuned.

\begin{figure}
  \centering
  % \resizebox{\columnwidth}{4cm}{%
  \input{figures/convergence.tex}
  % }
  \vspace{-3mm}
  \caption{Convergence of regularized logistic regression for $\lambda = 0.1$ and two
  different values of $\gamma$.}
  \label{fig:test}
\end{figure}


To choose the number of iterations, we plotted the training and validation errors at each epoch for all the algorithms and some hyperparameters.
One of these plots can
be seen in \autoref{fig:test}.
The errors seemed to have converged after 1000 iterations, so we decided to run all our grid-searches with a conservative value of 10 000 iterations.


This proved to be a difficult decision since setting a lower learning rate can lead to better results, if the number of iterations is high enough.
However, increasing the number of iterations can be time-consuming due to the number of combinations of hyperparameters.

\begin{figure}
  \centering
  % \resizebox{\columnwidth}{4cm}{%
  \input{figures/overfitting.tex}
  % }
  \vspace{-3mm}
  \caption{Validation error depending on degree of polynomial using closed-form least squares weights; adding more monomial terms leads to overfitting.}
  \label{fig:overfitting}
  \vspace{-4mm}
\end{figure}


During cross-validation we could observe that increasing the degree of the polynomial expansion leads to overfitting, as can be seen in \autoref{fig:overfitting}.

We chose the better hyperparameters according to the combination which yields the smallest average validation loss across the 4 folds.


\subsection{First Model}
For our first model, we took the dataset without the columns containing invalid values and trained it using all the different algorithms mentioned above.



% \begin{table}[h]
%     \begin{subtable}[h]{0.45\textwidth}
%         \centering
%         \begin{tabular}{ |c|c|c|c|c|c| } 
%             \hline
%              & LS & Ridge & LS SGD & Logistic & \makecell{Regularised  \\ Logistic } \\
%             \hline
%             % Validation & 0.0690 & 0.0710 & 0.0969 & 1.8184 & 0.6332 \\
%             Accuracy   & 0.815 & 0.806 & 0.734 & 0.714 & 0.714 \\
%             \hline
%         \end{tabular}
%         \caption{\texttt{NaN} replaced}
%         \label{tbl:nan-replaced}
%     \end{subtable}
%     \hfill
%     \begin{subtable}[h]{0.45\textwidth}
%         \centering
%             \begin{tabular}{ |c|c|c|c|c|c| } 
%                 \hline
%                  & LS & Ridge & LS SGD & Logistic & \makecell{Regularised  \\ Logistic } \\
%                 \hline
%                 % Validation & 0.0759 & 0.0785 & 0.0926 & 1.1623 & 0.5384 \\
%                 Accuracy   & 0.790 & 0.776 & 0.706 & 0.690 & 0.695 \\
%                 \hline
%             \end{tabular}
%         \caption{\texttt{NaN} removed}
%         \label{tbl:nan-removed}
%     \end{subtable}
%     \caption{Average validation error and accuracies over the folds, for each algorithm.}
%     \label{tbl:val-and-acc}
% \end{table}


\subsection{Second Model}
For the second model, we took the dataset where we replaced the invalid values by the mean of their corresponding feature and trained it using all the different algorithms described in \autoref{sec:algo}.

\subsection{Third Model}
For the last model, we divided our dataset into four parts according to the four categories of the unique categorical variable, \textsf{PRI\_jet\_num}.
We then removed the columns containing invalid values in each dataset. We think that training 4 different models and applying the corresponding one according to the value of \textsf{PRI\_jet\_num} can increase predictibility.

\begin{table}[h!]
  \centering
  % \textrm
  \begin{tabular}{ |c|c|c|c|c|c| } 
    \hline
     & LS & Ridge & LS SGD & Logistic & \makecell{Regularised  \\ Logistic } \\
    \hline
    \texttt{NaN} removed  & 0.790 & 0.776 & 0.706 & 0.690 & 0.695 \\
    \hline
    \texttt{NaN} replaced & 0.815 & 0.806 & 0.734 & 0.714 & 0.714 \\
    \hline
    \makecell{Categorised \& \\ \texttt{NaN} removed} & --- & 0.792 & --- & 0.711 & 0.709 \\
    \hline
  \end{tabular}
  % \vspace{-3mm}
  \caption{Best accuracy on test set for each data model.}
  \label{tbl:algo-acc}
\end{table}


%\begin{table}
%  \centering
  % \textrm
%  \begin{tabular}{ |c|c|c| } 
 %   \hline
 %   Ridge & Logistic & Reg. Logistic \\
%    \hline
 %   0.792 & 0.711    & 0.709 \\
%    \hline
%  \end{tabular}
 % % \vspace{-3mm}
%  \caption{Best accuracy on test set partitioned according to the categorical %variable. Note that Ridge regression achieves a much higher accuracy.}
%  \label{tbl:test-accuracies-partitioned}
%\end{table}


\section{Results}

After grid-searching for the best hyperparameters, it appears that ridge regression achieves the best results among all algorithms when used on the third model based on our accuracy and the results from AICrowd. 

The parameters used for each category are shown in \autoref{tbl:parameters-ridge-partitioned}. Using this model, we achieved an accuracy of 0.794 on AICrowd (ID: 92621).

% The value of $\lambda$ chosen corresponds to that found with grid-search,
% the results of which are visualised in %\autoref{fig:rr-grid-search}.

\begin{table}
  \centering
  % \textrm
  \begin{tabular}{ |c|c|c|c|c| } 
    \hline 
    Category   & 0  & 1  & 2  & 3  \\
    \hline
    Max degree & 14 & 15 & 12 & 15 \\
    \hline
  \end{tabular}
  % \vspace{-3mm}
  \caption{Parameters used to train the ridge regression that produced 79.2\% accuracy (cf. \autoref{tbl:algo-acc}, third line). The value of $\lambda$ used was $10^{-8}$.}
  \label{tbl:parameters-ridge-partitioned}
\end{table}

\section{Discussion}
There are a number of free variables that can influence the performance
of the training. The learning algorithm, the learning rate and regularisation
(when required), the maximum degree of the polynomial expansion...

This complexity lead us to make choices based on intuition; namely,
which bounds and precision to use for the grid searches.
Indeed, it is very possible that, with a lower learning rate and
different regularisations (provided that convergence is reached fast
enough), we could have achieved better results.
Unfortunately, compromises had to be made with respect to the 
time-performance trade-off, which may have lead to us to
miss very high-performing combinations of parameters.

Similarly, feature expansion was performed to a lesser degree than
what might have been required: we decided not to include interactions
terms in order to limit the number of features and the effect of
multicollinearity, and did not use any other nonlinear transformation
such as $\log$, $\exp$ or $\sin$.
In fact, doing this would probably allow us to lower the degree which is quite high (cf. \autoref{tbl:parameters-ridge-partitioned}).

In the third model, further work can be done to train the 4 different submodels in order to use different algorithms and expansion per submodel.
Additionally, we could also have replaced invalid
values by the column means, instead of removing them.
This further processing might yield better results,
according to the second line of \autoref{tbl:algo-acc}.


\section{Conclusion}
%Funny how degrees tend to not have an impact for SGD learning algorithms.
%Maybe training for longer would yield better results.

This project was a very good practical introduction to Machine Learning. We
managed to apply the techniques seen in class to obtain a model that was
satisfactory, even though the breadth of our explorations remained limited.
The biggest take-away from this project would be not to neglect exploration
of the data to see if nonlinear relationships can be extracted. That way,
nonlinear-looking data can be brought back to a linear setting, where
our methods can be applied to their full potential.

% \section*{Acknowledgements}

%\bibliographystyle{IEEEtran}
%\bibliography{literature}

\end{document}
